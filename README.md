# DOUBLE-BLIND PEER REVIEW VERSION
Please note that this is an anonymized version of the code presented in the paper "DeepHYDRA: Resource-Efficient Time-Series Anomaly Detection in Dynamically-Configured Systems".
The method was formerly dubbed "Synergising Transformer/DBSCAN Anomaly Detection", or "STRADA" in short.
We did our best to update the name where possible, and apologize if some remnants of the legacy name remain in this repository.
Any copyright notices identifying the authors or their organization have been redacted for the double-blind peer review.
We are kindly asking the reviewers receiving this code to refrain from sharing it with non-reviewers.

# DeepHYDRA
This is the official implementation of the DeepHYDRA algorithm presented in the paper "DeepHYDRA: Resource-Efficient Time-Series Anomaly Detection in Dynamically-Configured Systems".

## Creation of the conda Environments
The folder envs/ contains the conda environments for the different models.
You can create the conda environments with the following command:  

```
conda create --name <env_name> --file <file>  
```
In the respective conda environments, install the Python packages not installed via conda with  

```
pip install -r <env_name>_python_requirements.txt  
```
This is a bit messy, and we will probably streamline this in the future.

## Downloading/Creating the Datasets
For the machine-1-1 dataset, extract the files in the archive datasets/smd/machine-1-1.tar.gz.

For the HLT datasets, retrieve the original datasets from [here](https://zenodo.org/record/7908064) and place them in the subfolder datasets/hlt.
Afterwards, run the scripts  

```
generate_hlt_datasets.py  
generate_combined_detection_test_set.py  
```
Use the conda environment contained in envs/dataset_generation.txt for this step.

## Running the One-Liners
To run the one-liner baselines, run the script  

```
run_one_liners.sh  
```
in the subfolder baselines/one-liners.

## Running MERLIN
To run the MERLIN scripts, you have to clone the [py-merlin repository](https://gitlab.com/dlr-dw/py-merlin.git).
Build and install this package inside the environment contained in envs/merlin.txt.
Afterwards, you waill be able to run the script  

```
run_merlin.sh  
```

in the subfolder baselines/merlin.

## Running the Models on machine-1-1 and the Reduced HLT Datasets
Use the respectively named conda environments envs/informers.txt and envs/tranad.txt to run the specific models.
You can run the models with the parameters used in the paper by executing the scripts  

```
run_smd.sh  
run_hlt.sh  
run_hlt_unaugmented.sh
```

contained in the subfolder transformer_based_detection/informers and

```
run_smd_tranad.sh
run_smd_dagmm.sh
run_smd_usad.sh
run_smd_omni_anomaly.sh
run_hlt_tranad.sh
run_hlt_dagmm.sh
run_hlt_usad.sh
run_hlt_omni_anomaly.sh
run_hlt_unaugmented_tranad.sh
run_hlt_unaugmented_dagmm.sh
run_hlt_unaugmented_usad.sh
run_hlt_unaugmented_omni_anomaly.sh
```
contained in the subfolder transformer_based_detection/tranad.

## Running the Combined Detection
After training the models on the reduced HLT data, you can run the combined detection method using the generated checkpoints.
To do this, run the scripts  

```
run_informers.sh  
run.sh
```

contained in the subfolders detection_combined/benchmark/informers and detection_combined/benchmark/tranad respectively.

## Calculating the Performance Metrics and Generating the Plots
Running all of the scripts described above should have populated the folders evaluation/combined_detection/predictions, evaluation/reduced_detection/predictions, and evaluation/smd/predictions with the necessary files to calculate the performance metrics and generate the comparison plots shown in the paper.
You can calculate the metrics by running the scripts  

```
get_results_over_random_seeds.sh  
```

in the subfolders evaluation/combined_detection_dcm_2018, evaluation/reduced_detection_dcm_2018, and evaluation/smd.
Note that the results for models other than the Informer-based models on the machine-1-1 dataset are stored directly in the model folder as results_*_machine-1-1.csv.

The plots can be generated by running  

```
plot_comparison_plots.py  
```

in the subfolder evaluation/combined/detection.
